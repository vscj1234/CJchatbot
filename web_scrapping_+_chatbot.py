# -*- coding: utf-8 -*-
"""Web Scrapping + Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bbPWuT7V-s3i8yqnGSqxi1YGyJenLrii
"""

!pip install requests beautifulsoup4 html2text langchain chromadb openai schedule
!pip install python-dotenv  # For loading environment variables

# File: web_crawler.py

import requests
from bs4 import BeautifulSoup
import html2text

def get_data_from_website(url):
    response = requests.get(url)
    if response.status_code != 200:
        print("Failed to fetch the page")
        return "", {}
    soup = BeautifulSoup(response.content, 'html.parser')
    for script in soup(["script", "style"]):
        script.extract()
    html = str(soup)
    html2text_instance = html2text.HTML2Text()
    html2text_instance.images_to_alt = True
    html2text_instance.body_width = 0
    html2text_instance.single_line_break = True
    text = html2text_instance.handle(html)
    page_title = soup.title.string.strip() if soup.title else url.split("/")[-1]
    description = soup.find("meta", attrs={"name": "description"})
    description = description["content"] if description else page_title
    metadata = {
        'title': page_title,
        'url': url,
        'description': description,
        'keywords': ''
    }
    return text, metadata

# text_to_doc.py

import re
from langchain.text_splitter import MarkdownTextSplitter
from langchain.docstore.document import Document

def merge_hyphenated_words(text):
    return re.sub(r"(\w)-\n(\w)", r"\1\2", text)

def fix_newlines(text):
    return re.sub(r"(?<!\n)\n(?!\n)", " ", text)

def remove_multiple_newlines(text):
    return re.sub(r"\n{2,}", "\n", text)

def clean_text(text):
    cleaning_functions = [merge_hyphenated_words, fix_newlines, remove_multiple_newlines]
    for cleaning_function in cleaning_functions:
        text = cleaning_function(text)
    return text

def text_to_docs(text, metadata):
    doc_chunks = []
    text_splitter = MarkdownTextSplitter(chunk_size=2048, chunk_overlap=128)
    chunks = text_splitter.split_text(text)
    for chunk in chunks:
        doc = Document(page_content=chunk, metadata=metadata)
        doc_chunks.append(doc)
    return doc_chunks

def get_doc_chunks(text, metadata):
    text = clean_text(text)
    doc_chunks = text_to_docs(text, metadata)
    return doc_chunks

!pip install langchain
!pip install langchain-community
!pip install unstructured

# Commented out IPython magic to ensure Python compatibility.
# %pip install --q unstructured langchain
# %pip install --q "unstructured[all-docs]"

!pip install tiktoken -q
!pip install -U langchain-openai
!pip install faiss-cpu
!pip install streamlit

# utils.py
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings

def get_embeddings():
    return OpenAIEmbeddings()

def store_docs(url):
    text, metadata = get_data_from_website(url)
    if not text:
        return []
    docs = get_doc_chunks(text, metadata)
    return docs

def create_vector_store(documents):
    embeddings = get_embeddings()
    index = FAISS.from_documents(documents, embeddings)
    return index

from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI
import os

# Set your OpenAI API key
os.environ["OPENAI_API_KEY"] = "sk-proj-pXPuDxlpxwQmnL8qYeQzT3BlbkFJukfAwqbmMtlmFhQKNMKp"

# Initialize the language model
llm = OpenAI(temperature=0)

# Load the QA chain
chain = load_qa_chain(llm, chain_type="stuff")

# Function to get similar documents
def get_similar_docs(query, index, k=2, score=False):
    if score:
        similar_docs = index.similarity_search_with_score(query, k=k)
    else:
        similar_docs = index.similarity_search(query, k=k)
    return similar_docs

# Function to get the answer to a query
def get_answer(query, index):
    similar_docs = get_similar_docs(query, index)
    answer = chain({"input_documents": similar_docs, "question": query})
    return answer['output_text']

import schedule
import time

def job():
    url = "https://cloudjune.com/"  # Replace with your specific URL
    new_docs = store_docs(url)
    if new_docs:
        global index
        index = create_vector_store(new_docs)

# Schedule the job every 24 hours
schedule.every(24).hours.do(job)

# Initialize the vector store with initial data
initial_docs = store_docs("https://cloudjune.com/")
index = create_vector_store(initial_docs)

# Run the scheduler
while True:
    schedule.run_pending()
    time.sleep(1)

# Example usage
query = "Key features of Cloud June data Analytics"
answer = get_answer(query, index)
print(answer)

